{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8401fd-dd7b-4419-8bb7-a6be0e6ee625",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omegaconf, einops, pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7837c3cf-ea86-4117-8537-7b3c61dd03f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gfran\\anaconda3\\envs\\trch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from ldm.util import load_safetensors, get_input\n",
    "import torch\n",
    "from safetensors import safe_open\n",
    "from einops import rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce3d3c0-3a49-4f95-a411-d0b583d807bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gfran\\anaconda3\\envs\\trch\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "model = load_safetensors(ckpt='../stable-diffusion-models/vae/diffusion_pytorch_model.safetensors')\n",
    "\n",
    "# and then use it as model.encode(get_input(x), return_hidden=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e7569a7-96d6-4e10-a9df-100693899e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.as_tensor(np.zeros((1, 256, 256, 3), np.float32)).cuda()\n",
    "\n",
    "inp = get_input(inp)\n",
    "\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c7c66cf-fd70-4885-9c55-c6d48a639c7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 2.00 GiB total capacity; 1.51 GiB already allocated; 0 bytes free; 1.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _, h, hm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PhD\\stable-diffusion\\ldm\\models\\autoencoder.py:329\u001b[0m, in \u001b[0;36mAutoencoderKL.encode\u001b[1;34m(self, x, return_hidden)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 329\u001b[0m     h, hm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     moments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv(h)\n\u001b[0;32m    331\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m DiagonalGaussianDistribution(moments)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\PhD\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py:442\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_level \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_resolutions):\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_res_blocks):\n\u001b[1;32m--> 442\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_level\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_block\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown[i_level]\u001b[38;5;241m.\u001b[39mattn) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    444\u001b[0m             h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown[i_level]\u001b[38;5;241m.\u001b[39mattn[i_block](h)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\PhD\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py:131\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[1;34m(self, x, temb)\u001b[0m\n\u001b[0;32m    128\u001b[0m     h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemb_proj(nonlinearity(temb))[:,:,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    130\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(h)\n\u001b[1;32m--> 131\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[43mnonlinearity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(h)\n\u001b[0;32m    133\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h)\n",
      "File \u001b[1;32m~\\PhD\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py:35\u001b[0m, in \u001b[0;36mnonlinearity\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnonlinearity\u001b[39m(x):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# swish\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 2.00 GiB total capacity; 1.51 GiB already allocated; 0 bytes free; 1.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "_, h, hm = model.encode(inp, return_hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ccd95d0-45c9-46b3-b822-dbcef6c43cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 18, 18])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3966dd-7600-42ff-8057-b0f7bbfc052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoencoderKL(\n",
      "  (encoder): Encoder(\n",
      "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down): ModuleList(\n",
      "      (0): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (downsample): Downsample(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (1): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (downsample): Downsample(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (2): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (downsample): Downsample(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (3): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "      )\n",
      "    )\n",
      "    (mid): Module(\n",
      "      (block_1): ResnetBlock(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (attn_1): AttnBlock(\n",
      "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (block_2): ResnetBlock(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (mid): Module(\n",
      "      (block_1): ResnetBlock(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (attn_1): AttnBlock(\n",
      "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (block_2): ResnetBlock(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (up): ModuleList(\n",
      "      (0): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (2): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "      )\n",
      "      (1): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (2): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (upsample): Upsample(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (2): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (2): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (upsample): Upsample(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (3): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (2): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (upsample): Upsample(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (loss): Identity()\n",
      "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6fff958-b469-4619-9ed2-dbd1b5dfa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd9ac4e-800a-47e9-a7a0-6dbf1ffe83b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gfran\\anaconda3\\envs\\trch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ldm.util import load_safetensors, get_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f68381f-329b-47f2-a90a-6e998d3427a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gfran\\anaconda3\\envs\\trch\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "model = load_safetensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b86a385-a4ba-47c6-b3f1-ac5fd64b19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(np.zeros((1, 64, 64, 3), np.float32)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b468bde-5a07-432d-aac1-ad1cfa4bffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = get_input(inp)\n",
    "latent, outputs = model.encode(inp, return_hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb8ad639-9ae8-4274-be17-37625cc88c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2858, -0.4730, -0.5461,  ..., -0.1655, -0.3258,  0.0375],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c282979-0f3f-4de9-a397-c791b0dea394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(inp, layer=-1): # needs inp with 3 channels\n",
    "    inp = get_input(inp)\n",
    "    gd, outputs = model.encode(inp, return_hidden=True)\n",
    "    if layer > len(outputs):\n",
    "        # return np.reshape(np.stack(outputs.hidden_states)[:, :, 0], (1, -1))\n",
    "        result = torch.cat([torch.flatten(out) for out in outputs])\n",
    "    if layer == -1:\n",
    "        result = gd.sample()  # gd.mean()\n",
    "    else:\n",
    "        result = torch.flatten(outputs[layer])\n",
    "    return np.asarray(result.cpu())\n",
    "\n",
    "def get_features(inp, **args):\n",
    "    return get_image_features(inp, **args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trch",
   "language": "python",
   "name": "trch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
